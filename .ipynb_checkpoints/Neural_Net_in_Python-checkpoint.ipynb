{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [blog_link](https://eisenjulian.github.io/deep-learning-in-100-lines/)\n",
    "- [Pytorch nn module tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(low=0, high=10,size=(4,3))\n",
    "c = np.random.randint(low=0, high=10,size=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a, c).shape # n is 5, k is 3, m is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 75,  17,  29,  96],\n",
       "       [ 30,   8,  12,  33],\n",
       "       [135,  52,  66, 174],\n",
       "       [ 93,  51,  55, 117]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param Class\n",
    "\n",
    "We can start with a class that encapsulates \n",
    "- tensor\n",
    "- its gradients. \n",
    "\n",
    "The tensor can be anything like numpy array or torch array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "        self.gradient = np.zeros_like(self.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class\n",
    "\n",
    "Now we can create the layer class, the key idea is that during a forward pass (`Forward()`) we return both \n",
    "\n",
    "- layer output \n",
    "- `function`\n",
    "\n",
    "The `function` will have the following signature:\n",
    "\n",
    "```py\n",
    "def function(input):\n",
    "    \"\"\"\n",
    "    input (tensor): gradient of the loss with respect to the outputs\n",
    "    return (tensor): the gradient with respect to the inputs, \n",
    "                    updating the weight gradients in the process\n",
    "    \"\"\"\n",
    "    output = do_operation(input)\n",
    "    return output\n",
    "\n",
    "```\n",
    "\n",
    "This is because while evaluating the model layer by layer there's no way to calculate the gradients if we don't know the final loss yet, instead the best thing you can do is return a function that CAN calculate the gradient later. And that function will only be called after we completed the forward evaluation, when you know the loss and you have all the necessary info to compute the gradients in that layer.\n",
    "\n",
    "\n",
    "The training process will then have three steps, calculate the forward step, then the backward steps accumulate the gradients, and finally updating the weights. It’s important to do this at the end\tsince weights can be reused in multiple layers and we don’t want to mutate the weights before time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X, lambda D: D\n",
    "\n",
    "    def build_param(self, tensor):\n",
    "        \"\"\"\n",
    "        tensor (numpy matrix)\n",
    "        return:\n",
    "            param (Parameter)\n",
    "        \"\"\"\n",
    "        param = Parameter(tensor)\n",
    "        self.parameters.append(param)\n",
    "        return param\n",
    "    \n",
    "    def update(self, optimizer):\n",
    "        for param in self.parameters: optimizer.update(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's standard to delegate the job of updating the parameters to an optimizer, which receives an instance of a parameter after every batch. The simplest and most known optimization method out there is the mini-batch stochastic gradient descent\n",
    "\n",
    "### Code Analysis\n",
    "Also look at line 6, where it's returning a lambda. Here it's returning a lambda function that can be applied later with the appropriate argument. \n",
    "\n",
    "How a returned lambda function can work? \n",
    "\n",
    "Lets see an example\n",
    "\n",
    "```py\n",
    "def fun(n):\n",
    "    \"\"\"\n",
    "    return a lambda function with the increment size\n",
    "    \"\"\"\n",
    "    return lambda x: x+n\n",
    "\n",
    "x = 3\n",
    "inc3 = fun(3)\n",
    "inc3(x) ## this will increase the x's value by 3\n",
    "ans: 6\n",
    "\n",
    "inc5 = fun(5)\n",
    "inc5(x) ## this will increase the x's value by 3\n",
    "ans: 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer():\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, param):\n",
    "        \"\"\"\n",
    "        the tensors are updated, but not the gradients. they are filled with 0. Why ????\n",
    "        look at the backward() definition in Class Linear. In the backward(), the gradients are updated.\n",
    "        \"\"\"\n",
    "        param.tensor -= self.lr * param.gradient\n",
    "        param.gradient.fill(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next build our `Linear Layer` extending the `Class Layer`\n",
    "\n",
    "## Linear Class\n",
    "\n",
    "For reference let's look at the `Layer class`, which the `Linear class` is extending\n",
    "```py\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.parameters = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X, lambda D: D\n",
    "\n",
    "    def build_param(self, tensor):\n",
    "        \"\"\"\n",
    "        tensor (numpy matrix)\n",
    "        return:\n",
    "            param (Parameter)\n",
    "        \"\"\"\n",
    "        param = Parameter(tensor)\n",
    "        self.parameters.append(param)\n",
    "        return param\n",
    "    \n",
    "    def update(self, optimizer):\n",
    "        for param in self.parameters: optimizer.update(param)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        inputs (int): input dimension\n",
    "        outputs (int): output dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        tensor = np.random.randn(inputs, outputs) * np.sqrt(1 / inputs)\n",
    "        self.weights = self.build_param(tensor)\n",
    "        self.bias = self.build_param(np.zeros(outputs))\n",
    "\n",
    "    def forward(self, X):\n",
    "        def backward(D):\n",
    "            self.weights.gradient += X.T @ D\n",
    "            self.bias.gradient += D.sum(axis=0)\n",
    "            return D @ self.weights.tensor.T\n",
    "        \n",
    "        return X @ self.weights.tensor +  self.bias.tensor, backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Analysis\n",
    "\n",
    "**line 8**\n",
    "\n",
    "We are initializing the weights here with `Xavier initialisation` (by multiplying with `1/sqrt(n)`), where n is the input dimension\n",
    "\n",
    "**Line 9, 10**\n",
    "\n",
    "the build param is returning a Parameter object which has a tensor and the gradient. The tensor is filled with the input tensor. But the gradient is filled with `0` intially.\n",
    "\n",
    "**Line 14-16**\n",
    "\n",
    "the `backward()` definition is something similar to \n",
    "\n",
    "```py\n",
    "def outfun(x):\n",
    "    def infun(n):\n",
    "        print('process infun() running ...')\n",
    "        return x*n\n",
    "    return 5*x, infun\n",
    "\n",
    "x = 10\n",
    "z, y = outfun(x)\n",
    "\n",
    "y(2)\n",
    "Ans:\n",
    "   process infun() running ...\n",
    "   20\n",
    "```\n",
    "\n",
    "Here the logic is outfun will return the `5*x` and also the **partial funciton defition** for `infun()`. Why **partial**?: Because when the definition of `infun(n)` is generated, it doesn't know the `n`. But it receives the `x` from `outfun(x)`. So `infun()` definition is partially generated when `outfun()` returns `infun`. Later, when we call `y(2)` then `y == infun` and `n == 2` and then `infun(2)` is executed and it gives `20`. It means that the `infun()` definition is not static. Rather it's dynamic. Because it's one part is bound to the input of the `outfun()`. Thus for different `outfun(10)`, `outfun(20)` call, it's `infun(2)` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outfun(x):\n",
    "    def infun(n):\n",
    "        print('process infun() running ...')\n",
    "        return x*n\n",
    "    return 5*x, infun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10\n",
    "z, y = outfun(x)\n",
    "\n",
    "y(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
